#Question2

from sklearn.preprocessing import MinMaxScaler
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_absolute_error

# Load dataset
diabetes = load_diabetes()
X = diabetes.data
y = diabetes.target

# Scale the target variable
scaler = MinMaxScaler()
y = y.reshape(-1, 1)
y = scaler.fit_transform(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)


model_without_relu = MLPRegressor(hidden_layer_sizes=(8,), activation='relu', max_iter=10, random_state=42)
model_with_relu = MLPRegressor(hidden_layer_sizes=(8,), activation='relu', max_iter=10, random_state=42)

# Train the models
model_without_relu.fit(X_train, y_train)
model_with_relu.fit(X_train, y_train)

# Predictions
y_pred_without_relu = model_without_relu.predict(X_test)
y_pred_with_relu = model_with_relu.predict(X_test)


y_pred_without_relu = scaler.inverse_transform(y_pred_without_relu.reshape(-1, 1))
y_pred_with_relu = scaler.inverse_transform(y_pred_with_relu.reshape(-1, 1))
y_test = scaler.inverse_transform(y_test)

# Calculate mean absolute error for both models
mae_without_relu = mean_absolute_error(y_test, y_pred_without_relu)
mae_with_relu = mean_absolute_error(y_test, y_pred_with_relu)

print(f"Mean Absolute Error without relu: {mae_without_relu}")
print(f"Mean Absolute Error with relu: {mae_with_relu}")
#2. Is the ‘relu’ activation function appropriate for the output layer of this model?
#Since the target variable is continuous (representing diabetes progression), 'relu' might not be the best choice for the output layer
